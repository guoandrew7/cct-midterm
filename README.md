# cct-midterm

Using the Cultural Consensus Theory, I modeled each informants relative competence based on the question set and found a 'consensus answer key' for said questions. When determining priors, I decided to use D = pm.Uniform("D", lower=0.5, upper=1.0, shape=N) for competence. The bounds are set to 0.5 and 1 because the worst possible competence is at chance level (0.5), and the higest possible competence is getting everything right (1). I also decided to use a uniform distribution because of the context of the data. Given that the data is largely unexplained, I cannot assume that there is any underlying bias towards a different score, such as an easy topic potentially using a normal distribution centered around 0.75, so I decided to use a uniform distribution to limit external biases. I decided to set Z = pm.Bernoulli("Z", p=0.5, shape=M) because I was given no information about which questions had which correct answers, so it makes sense to randomly randomly pick between 0 and 1 at equal likelihoods. When viewing the posterior diagnostics, each question reached a r_hat value of 1, which means that each question succesfully converged.


The model resulted in each of the 10 informants having posterior mean competences above 0.5, with informant 6 being the most competent  at 0.873 and informant 3 the least at 0.561. This means that all 10 informants were likely not guessing as they are all above chance probability. The resulting mean probabilitys for each consensus answer were all between 0 and 1, with values near the extremes signifying that the questions were extremely agreed upon, while questions with probabilities closer to 0.5 were not as clear cut. One issue I came across while programming was being unable to display the graphs directly into the console when running the program, so the graph outputs are saves as png files within the directory as a bandaid fix. Finally, the differences that occur between the consensus means and the majority votes are due to the MCMC sampler's preference for higher competence informants while the majority vote weights each informant equally. This leads to differences due to wrong answers having a smaller impact in the model but a larger impact in the majority vote. For example, if the 9 most competent informants pick true for a question while the other 11 less competent informants pick false, the model will determine that the consensus should be true while the majority will result in false.
